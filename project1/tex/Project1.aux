\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\gdef \tocmax@section{14.77989pt}
\gdef \tocmax@subsection{5.0pt}
\gdef \tocmax@subsubsection{5.0pt}
\gdef \tocmax@paragraph{5.0pt}
\gdef \tocmax@appendix{5.0pt}
\gdef \tocmax@pagenum{10.0pt}
\@writefile{toc}{\tocdepth@munge}
\@writefile{toc}{\contentsline {section}{\numberline {}Contents}{0}{section*.1}\protected@file@percent }
\@writefile{toc}{\tocdepth@restore}
\@writefile{toc}{\contentsline {section}{\numberline {}Introduction}{1}{section*.2}\protected@file@percent }
\newlabel{eq:FF}{{1}{1}{}{equation.0.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Exercise 1: Ordinary Least Square (OLS) on the Franke function}{1}{section*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Here we have displayed our noisy data along with the analytical function. The left figure shows the analytical Franke function, while the right figure is the Franke function with noise.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:FF_dataset}{{1}{2}{Here we have displayed our noisy data along with the analytical function. The left figure shows the analytical Franke function, while the right figure is the Franke function with noise}{figure.1}{}}
\newlabel{eq:zpolynm}{{2}{2}{}{equation.0.2}{}}
\newlabel{eq:z_true_data}{{3}{2}{}{equation.0.3}{}}
\newlabel{eq:Const_OLS}{{4}{3}{}{equation.0.4}{}}
\newlabel{eq:OLS_beta}{{5}{3}{}{equation.0.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Here we have plotted the MSE and $R^2$-score for OLS. The left figure shows the MSE and right shows the $R^2$ score. The red dotted line is from the train data, and the blue dotted line is from the test data.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:OLS_R2_and_MSE}{{2}{4}{Here we have plotted the MSE and $R^2$-score for OLS. The left figure shows the MSE and right shows the $R^2$ score. The red dotted line is from the train data, and the blue dotted line is from the test data}{figure.2}{}}
\newlabel{eq:var_OLS}{{6}{4}{}{equation.0.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Here we have plotted the ideal parameters, for different polynomial degrees and with the confidence intervals. Top left figure has polynomial degree $p = 1$, top right $p = 3$ and bottom left $p = 5$. All the plots are of $\bm  {\beta }$ as a function of the index in the vector. The confidence interval is $\pm 2\bm  {\sigma }(\bm  {\hat  {\beta }}_{j})$, where $\bm  {\sigma }(\bm  {\hat  {\beta }}_{j})$ is given by equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:var_OLS}\unskip \@@italiccorr )}}.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:var_beta}{{3}{5}{Here we have plotted the ideal parameters, for different polynomial degrees and with the confidence intervals. Top left figure has polynomial degree $p = 1$, top right $p = 3$ and bottom left $p = 5$. All the plots are of $\boldsymbol {\beta }$ as a function of the index in the vector. The confidence interval is $\pm 2\boldsymbol {\sigma }(\boldsymbol {\hat {\beta }}_{j})$, where $\boldsymbol {\sigma }(\boldsymbol {\hat {\beta }}_{j})$ is given by equation \eqref {eq:var_OLS}}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Exercise 2: Bias-variance trade-off and bootstrapping}{5}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Test MSE for OLS with $B=90$ iterations with the Bootstrap resampling method, using $p_\text  {max}=20$ on the left panel and $p_\text  {max}=10$ on the right panel.}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:OLS_overfitting}{{4}{6}{Test MSE for OLS with $B=90$ iterations with the Bootstrap resampling method, using $p_\text {max}=20$ on the left panel and $p_\text {max}=10$ on the right panel}{figure.4}{}}
\newlabel{eq:bias_var_tradeoff}{{7}{6}{}{equation.0.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Here we have plotted the bias, variance and MSE for the test and train data. The $x$-axis shows the polynomial degree, and the $y$-axis indicates the bias, variance and MSE. The dotted lines are for the train data and solid lines are the test data.}}{7}{figure.5}\protected@file@percent }
\newlabel{fig:BV_OLS}{{5}{7}{Here we have plotted the bias, variance and MSE for the test and train data. The $x$-axis shows the polynomial degree, and the $y$-axis indicates the bias, variance and MSE. The dotted lines are for the train data and solid lines are the test data}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Exercise 3: Cross-validation as resampling technique, adding more complexity}{8}{section*.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Here we have plotted the MSE when performing cross-validation as the resampling technique. On the $x$-axis we have polynomial degree, and MSE on the $y$-axis. The top left, right and bottom left figures are with $k=[5, 7, 10]$ iterations respectively. The bottom left plot is to illustrate the diverging MSE for higher polynomial degree, which we see irregardless of iteration-number.}}{9}{figure.6}\protected@file@percent }
\newlabel{fig:CV_OLS}{{6}{9}{Here we have plotted the MSE when performing cross-validation as the resampling technique. On the $x$-axis we have polynomial degree, and MSE on the $y$-axis. The top left, right and bottom left figures are with $k=[5, 7, 10]$ iterations respectively. The bottom left plot is to illustrate the diverging MSE for higher polynomial degree, which we see irregardless of iteration-number}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Exercise 4: Ridge Regression on the Franke function with resampling}{9}{section*.6}\protected@file@percent }
\newlabel{eq:beta_ridge_ols}{{8}{10}{}{equation.0.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Test MSE using Ridge and bootstrapping, as function of polynomial degree and $\lambda $. The minimum value is $MSE=0.0472$ at $p=8$ and $log10(\lambda _{min}) = -2.07$, indicated by the red cross.}}{10}{figure.7}\protected@file@percent }
\newlabel{fig:Ridge-boot_heatmap}{{7}{10}{Test MSE using Ridge and bootstrapping, as function of polynomial degree and $\lambda $. The minimum value is $MSE=0.0472$ at $p=8$ and $log10(\lambda _{min}) = -2.07$, indicated by the red cross}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MSE using Ridge with cross-validation, k=10, as function of $\lambda $ and polynomial degree. The minimum value is $MSE=0.0493$ at $p=3$ and $log10(\lambda )=-2.24$)}}{12}{figure.8}\protected@file@percent }
\newlabel{fig:Ridge_CV_heatmap}{{8}{12}{MSE using Ridge with cross-validation, k=10, as function of $\lambda $ and polynomial degree. The minimum value is $MSE=0.0493$ at $p=3$ and $log10(\lambda )=-2.24$)}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Exercise 5: Lasso regression on the Franke function with resampling}{12}{section*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MSE, bias and variance using Ridge and bootstrapping, for different $\lambda $s.}}{13}{figure.9}\protected@file@percent }
\newlabel{fig:Ridge_boot_BVT}{{9}{13}{MSE, bias and variance using Ridge and bootstrapping, for different $\lambda $s}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces MSE using Lasso with bootstrapping, as function of $\lambda $ and polynomial degrees. The lowest error is $MSE_{min}=0.0473$, for $p=7$ and $\lambda = 10^{-5.0}$.}}{14}{figure.10}\protected@file@percent }
\newlabel{fig:Lasso_boot_heatmap}{{10}{14}{MSE using Lasso with bootstrapping, as function of $\lambda $ and polynomial degrees. The lowest error is $MSE_{min}=0.0473$, for $p=7$ and $\lambda = 10^{-5.0}$}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Comparing Regression methods}{14}{section*.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Lowest MSE for the various regression methods with resampling, using $B=90$ bootstrap iterations and $10$ $k$-folds}}{14}{table.1}\protected@file@percent }
\newlabel{tab:franke_mse}{{I}{14}{Lowest MSE for the various regression methods with resampling, using $B=90$ bootstrap iterations and $10$ $k$-folds}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Here we have plotted the bias, variance and MSE as a function of polynomial degree, and for lambda values $\lambda = 10^{-8}, 10^{-4}, 10^{-2}$.}}{15}{figure.11}\protected@file@percent }
\newlabel{fig:Lasso_BVT}{{11}{15}{Here we have plotted the bias, variance and MSE as a function of polynomial degree, and for lambda values $\lambda = 10^{-8}, 10^{-4}, 10^{-2}$}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces MSE using Lasso and cross-validation, for a wide range of $\lambda $ and polynomial degrees. The upper plot is with $log10(MSE)$, while on the bottom is just the $MSE$.}}{16}{figure.12}\protected@file@percent }
\newlabel{fig:Lasso_CV_heatmaps}{{12}{16}{MSE using Lasso and cross-validation, for a wide range of $\lambda $ and polynomial degrees. The upper plot is with $log10(MSE)$, while on the bottom is just the $MSE$}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}Exercise 6: Analysis of real data}{17}{section*.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Here we have plotted the terrain data which we will use in this exercise.}}{17}{figure.13}\protected@file@percent }
\newlabel{fig:terrain_raw}{{13}{17}{Here we have plotted the terrain data which we will use in this exercise}{figure.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}OLS}{18}{section*.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Her we have plotted the $\log (\text  {MSE})$ as a function of polynomial degree. The left plot is with $(50\cross 50)$ grid-points and right with $(30\cross 30)$.}}{18}{figure.14}\protected@file@percent }
\newlabel{fig:terrain_OLS_MSE_bootstrap}{{14}{18}{Her we have plotted the $\log (\text {MSE})$ as a function of polynomial degree. The left plot is with $(50\cross 50)$ grid-points and right with $(30\cross 30)$}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Data til venstre. Fit til høyre}}{19}{figure.15}\protected@file@percent }
\newlabel{fig:terrain_fit}{{15}{19}{Data til venstre. Fit til høyre}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Here we have plotted $\log (\text  {MSE})$ as a function of polynomial degree, with cross-validation as the resampling technique. Left graph shows with $k=5$ iterations, and right with $k=10$.}}{20}{figure.16}\protected@file@percent }
\newlabel{fig:terrain_OLS_MSE_CV}{{16}{20}{Here we have plotted $\log (\text {MSE})$ as a function of polynomial degree, with cross-validation as the resampling technique. Left graph shows with $k=5$ iterations, and right with $k=10$}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Here we have plotted the logarithm of MSE, variance and bias, as a function of polynomial degree. We used $B=50$ bootstrap iterations. The left plot is with $(50\cross 50)$ grid-points and right with $(30\cross 30)$.}}{20}{figure.17}\protected@file@percent }
\newlabel{fig:terrain_OLS_BVT}{{17}{20}{Here we have plotted the logarithm of MSE, variance and bias, as a function of polynomial degree. We used $B=50$ bootstrap iterations. The left plot is with $(50\cross 50)$ grid-points and right with $(30\cross 30)$}{figure.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Ridge}{20}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces This contour shows the MSE (colorbar), when using bootstrapping, as a function of polynomial degree ($x$-axis) and $\log (\lambda )$ ($y$-axis). The red X marks for what values of $p$ and $\lambda $ we have the minimum MSE.}}{21}{figure.18}\protected@file@percent }
\newlabel{fig:terrain_Ridge_MSE_Boot}{{18}{21}{This contour shows the MSE (colorbar), when using bootstrapping, as a function of polynomial degree ($x$-axis) and $\log (\lambda )$ ($y$-axis). The red X marks for what values of $p$ and $\lambda $ we have the minimum MSE}{figure.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Lasso}{21}{section*.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces This contour shows the MSE (colorbar), when using cross-validation, as a function of polynomial degree ($x$-axis) and $\log (\lambda )$ ($y$-axis). The red X marks for what values of $p$ and $\lambda $ we have the minimum MSE.}}{22}{figure.19}\protected@file@percent }
\newlabel{fig:terrain_Ridge_MSE_CV}{{19}{22}{This contour shows the MSE (colorbar), when using cross-validation, as a function of polynomial degree ($x$-axis) and $\log (\lambda )$ ($y$-axis). The red X marks for what values of $p$ and $\lambda $ we have the minimum MSE}{figure.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Analysis}{22}{section*.13}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces In this table we have listed the strictly best MSE-values for the given methods and resampling technique. }}{22}{table.2}\protected@file@percent }
\newlabel{tab:anal6}{{II}{22}{In this table we have listed the strictly best MSE-values for the given methods and resampling technique}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces This contour shows the MSE (colorbar), when using bootstrapping, as a function of polynomial degree ($x$-axis) and $\log (\lambda )$ ($y$-axis). The red X marks for what values of $p$ and $\lambda $ we have the minimum MSE.}}{23}{figure.20}\protected@file@percent }
\newlabel{fig:terrain_Lasso_MSE_Boot}{{20}{23}{This contour shows the MSE (colorbar), when using bootstrapping, as a function of polynomial degree ($x$-axis) and $\log (\lambda )$ ($y$-axis). The red X marks for what values of $p$ and $\lambda $ we have the minimum MSE}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces This contour shows the MSE (colorbar), when using cross-validation, as a function of polynomial degree ($x$-axis) and $\log (\lambda )$ ($y$-axis). The red X marks for what values of $p$ and $\lambda $ we have the minimum MSE.}}{23}{figure.21}\protected@file@percent }
\newlabel{fig:terrain_Lasso_MSE_CV}{{21}{23}{This contour shows the MSE (colorbar), when using cross-validation, as a function of polynomial degree ($x$-axis) and $\log (\lambda )$ ($y$-axis). The red X marks for what values of $p$ and $\lambda $ we have the minimum MSE}{figure.21}{}}
\@writefile{toc}{\appendix }
\@writefile{toc}{\contentsline {section}{\numberline {A}Bias-variance Decomposition}{24}{section*.14}\protected@file@percent }
\newlabel{Apx:BVT}{{A}{24}{}{section*.14}{}}
\bibdata{Project1Notes}
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\newlabel{LastPage}{{}{25}{}{}{}}
\gdef \@abspage@last{26}
