\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{2019}
\citation{project1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Gradient Descent}{1}{subsection.2.1}\protected@file@percent }
\newlabel{sec:GD}{{2.1}{1}{Gradient Descent}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Ordinary Gradient Descent}{2}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{eq:Total_gradient}{{2.2}{2}{Ordinary Gradient Descent}{equation.2.2}{}}
\newlabel{eq:GD_algo}{{2.3}{2}{Ordinary Gradient Descent}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Stochastic Gradient Descent}{2}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{sec:SGD}{{2.1.2}{2}{Stochastic Gradient Descent}{subsubsection.2.1.2}{}}
\newlabel{eq:SGD_algo}{{2.5}{3}{Stochastic Gradient Descent}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Adding Momentum}{3}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{eq:vt_mom}{{2.6}{3}{Adding Momentum}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Logistic Regression}{3}{subsection.2.2}\protected@file@percent }
\newlabel{sec:logistic_regression}{{2.2}{3}{Logistic Regression}{subsection.2.2}{}}
\newlabel{eq:log_reg_model}{{2.7}{3}{Logistic Regression}{equation.2.7}{}}
\newlabel{eq:cross_entropy2}{{2.8}{4}{Logistic Regression}{equation.2.8}{}}
\newlabel{eq:cross_entropy_grad}{{2.9}{4}{Logistic Regression}{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Feed-Forward Deep Neural Networks}{4}{subsection.2.3}\protected@file@percent }
\newlabel{sec:NeuralNetwork}{{2.3}{4}{Feed-Forward Deep Neural Networks}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Architecture of Neural Networks}{4}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Basic outline of a neural network. It displays the different layers (input, hidden and output), nodes (gray circles) and the connection between the nodes (black lines).\relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:neural_network}{{1}{5}{Basic outline of a neural network. It displays the different layers (input, hidden and output), nodes (gray circles) and the connection between the nodes (black lines).\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Activation Functions}{5}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{sec:teo:act_funcs}{{2.3.2}{5}{Activation Functions}{subsubsection.2.3.2}{}}
\newlabel{eq:sigmoid}{{2.11}{5}{Activation Functions}{equation.2.11}{}}
\newlabel{eq:RELU}{{2.12}{5}{Activation Functions}{equation.2.12}{}}
\newlabel{eq:leaky_RELU}{{2.13}{5}{Activation Functions}{equation.2.13}{}}
\citation{project1}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Some activation functions, sigmoid, RELU and Leaky RELU, respectively.\relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:activation_functions}{{2}{6}{Some activation functions, sigmoid, RELU and Leaky RELU, respectively.\relax }{figure.caption.3}{}}
\newlabel{eq:softmax}{{2.14}{6}{Activation Functions}{equation.2.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Cost Function and Regularization}{6}{subsubsection.2.3.3}\protected@file@percent }
\newlabel{sec:cost_func_and_regul}{{2.3.3}{6}{Cost Function and Regularization}{subsubsection.2.3.3}{}}
\newlabel{eq:MSE}{{2.15}{6}{Cost Function and Regularization}{equation.2.15}{}}
\newlabel{eq:L1}{{2.16}{6}{Cost Function and Regularization}{equation.2.16}{}}
\newlabel{eq:L2}{{2.17}{6}{Cost Function and Regularization}{equation.2.17}{}}
\newlabel{eq:cross_entropy}{{2.23}{7}{Cost Function and Regularization}{equation.2.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}The Backpropagation Algorithm}{7}{subsubsection.2.3.4}\protected@file@percent }
\newlabel{sec:back_prop}{{2.3.4}{7}{The Backpropagation Algorithm}{subsubsection.2.3.4}{}}
\newlabel{eq:activation_of_node_ajl}{{2.24}{8}{The Backpropagation Algorithm}{equation.2.24}{}}
\newlabel{eq:backprop0}{{2.25}{8}{The Backpropagation Algorithm}{equation.2.25}{}}
\newlabel{eq:backprop1}{{2.26}{8}{The Backpropagation Algorithm}{equation.2.26}{}}
\newlabel{eq:backprop2}{{2.27}{8}{The Backpropagation Algorithm}{equation.2.27}{}}
\newlabel{eq:backprop3}{{2.28}{8}{The Backpropagation Algorithm}{equation.2.28}{}}
\newlabel{eq:backprop4}{{2.29}{8}{The Backpropagation Algorithm}{equation.2.29}{}}
\citation{xavier}
\citation{He}
\newlabel{eq:update_weights}{{2.30}{9}{The Backpropagation Algorithm}{equation.2.30}{}}
\newlabel{eq:update_biases}{{2.31}{9}{The Backpropagation Algorithm}{equation.2.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.5}Numerical gradients}{9}{subsubsection.2.3.5}\protected@file@percent }
\newlabel{sec:num_gradients}{{2.3.5}{9}{Numerical gradients}{subsubsection.2.3.5}{}}
\newlabel{eq:softmax_crossentropy_derivative}{{2.32}{9}{Numerical gradients}{equation.2.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.6}Initialization of weights}{9}{subsubsection.2.3.6}\protected@file@percent }
\newlabel{sec:wi}{{2.3.6}{9}{Initialization of weights}{subsubsection.2.3.6}{}}
\citation{project1}
\citation{project1}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{10}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Franke Function}{10}{subsection.3.1}\protected@file@percent }
\newlabel{eq:FF}{{3.1}{10}{Franke Function}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Stochastic Gradient Descent}{10}{subsubsection.3.1.1}\protected@file@percent }
\citation{project1}
\newlabel{eq:SGD_gradient}{{3.2}{11}{Stochastic Gradient Descent}{equation.3.2}{}}
\newlabel{eq:learning_schedule}{{3.3}{12}{Stochastic Gradient Descent}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Feed Forward Neural Network}{12}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{eq:R2_score}{{3.4}{13}{Feed Forward Neural Network}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Wisconsin Breast Cancer Data}{13}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Feed Forward Neural Network}{14}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Logistic Regression}{14}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{15}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Franke Function}{15}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Stochastic Gradient Descent}{15}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Initial MSE of the train and test data as a function of epochs for different learning rates $\eta $. A maximum MSE value of $0.07$ is set, so dark red regions may correspond to significantly higher MSE than it appears to.\relax }}{16}{figure.caption.4}\protected@file@percent }
\newlabel{fig:SGD_Franke_epochs_eta_overfit}{{3}{16}{Initial MSE of the train and test data as a function of epochs for different learning rates $\eta $. A maximum MSE value of $0.07$ is set, so dark red regions may correspond to significantly higher MSE than it appears to.\relax }{figure.caption.4}{}}
\citation{project1}
\citation{project1}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Test MSE for different $\eta $ values. $\eta <0.25$ gives a relatively slowly converging MSE, while $\eta >0.25$ gives fluctuating MSE values. Maximum MSE$=0.07$ is chosen.\relax }}{17}{figure.caption.5}\protected@file@percent }
\newlabel{fig:SGD_Franke_epochs_eta}{{4}{17}{Test MSE for different $\eta $ values. $\eta <0.25$ gives a relatively slowly converging MSE, while $\eta >0.25$ gives fluctuating MSE values. Maximum MSE$=0.07$ is chosen.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Test MSE for different number of minibatches. The bottom line corresponds to no minibatches overall, and clearly gives the worst result. The heatmap is produced with $\mathrm  {MSE}_\mathrm  {max}=0.07$.\relax }}{18}{figure.caption.6}\protected@file@percent }
\newlabel{fig:SGD_Franke_epochs_minibatches}{{5}{18}{Test MSE for different number of minibatches. The bottom line corresponds to no minibatches overall, and clearly gives the worst result. The heatmap is produced with $\mathrm {MSE}_\mathrm {max}=0.07$.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces MSE for different values $\lambda $, corresponding to Ridge regression, for different $\eta $ values.\relax }}{18}{figure.caption.7}\protected@file@percent }
\newlabel{fig:SGD_Franke_lambda_eta}{{6}{18}{MSE for different values $\lambda $, corresponding to Ridge regression, for different $\eta $ values.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces MSE evolution for different momentum parameters, using $\mathrm  {MSE}_\mathrm  {max}=0.07$, as before. The bottom line corresponds to no momentum.\relax }}{19}{figure.caption.8}\protected@file@percent }
\newlabel{fig:SGD_Franke_epochs_gamma}{{7}{19}{MSE evolution for different momentum parameters, using $\mathrm {MSE}_\mathrm {max}=0.07$, as before. The bottom line corresponds to no momentum.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Feed Forward Neural Network}{19}{subsubsection.4.1.2}\protected@file@percent }
\citation{project1}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Evolution of the MSE for different values of $\eta _0$, all of them decreasing linearly to a final value of $\eta =0$ after $150$ epochs.\relax }}{20}{figure.caption.9}\protected@file@percent }
\newlabel{fig:SGD_Franke_epochs_dynamic_eta}{{8}{20}{Evolution of the MSE for different values of $\eta _0$, all of them decreasing linearly to a final value of $\eta =0$ after $150$ epochs.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MSE of the train data during $150$ epochs for different learning rates using a Neural network.\relax }}{21}{figure.caption.10}\protected@file@percent }
\newlabel{fig:NN_Franke_epochs_eta_MSE}{{9}{21}{MSE of the train data during $150$ epochs for different learning rates using a Neural network.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Test MSE of the Franke function for different learning rates, during training.\relax }}{21}{figure.caption.11}\protected@file@percent }
\newlabel{fig:NN_Franke_epochs_eta_MSE_limited}{{10}{21}{Test MSE of the Franke function for different learning rates, during training.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The $R^2$ score of the Neural network, using a minimum value of the heatmap of $R^2_\mathrm  {min}=0.55$.\relax }}{22}{figure.caption.12}\protected@file@percent }
\newlabel{fig:NN_Franke_epochs_eta_R2}{{11}{22}{The $R^2$ score of the Neural network, using a minimum value of the heatmap of $R^2_\mathrm {min}=0.55$.\relax }{figure.caption.12}{}}
\citation{project1}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces MSE of the Franke function from the Neural network, using a linearly decreasing learning rate, where the initial learning rates are shown on the $y$-axis.\relax }}{23}{figure.caption.13}\protected@file@percent }
\newlabel{fig:NN_Franke_epochs_dynamic_eta}{{12}{23}{MSE of the Franke function from the Neural network, using a linearly decreasing learning rate, where the initial learning rates are shown on the $y$-axis.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces MSE of the Franke function with Relu activation for the hidden layers in the upper panel. The resulting obtained with Leaky Relu is shown in the bottom panel.\relax }}{24}{figure.caption.14}\protected@file@percent }
\newlabel{fig:NN_Franke_epochs_eta_MSE_relu_leaky}{{13}{24}{MSE of the Franke function with Relu activation for the hidden layers in the upper panel. The resulting obtained with Leaky Relu is shown in the bottom panel.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces MSE of the Neural network with the sigmoid activation function for the hidden layers. The learning rates and epochs are identical to that of figure \ref  {fig:NN_Franke_epochs_eta_MSE_relu_leaky}, allowing us to compare the effect of the different activation functions.\relax }}{25}{figure.caption.15}\protected@file@percent }
\newlabel{fig:NN_Franke_epochs_eta_MSE_sigmoid_compare}{{14}{25}{MSE of the Neural network with the sigmoid activation function for the hidden layers. The learning rates and epochs are identical to that of figure \ref {fig:NN_Franke_epochs_eta_MSE_relu_leaky}, allowing us to compare the effect of the different activation functions.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Neural networks with different number of layers, but same number of total nodes. Top left has 1 hidden layer, top right has 2. Bottom left has 3, while bottom right has 4. All have a total of 20 nodes.\relax }}{25}{figure.caption.16}\protected@file@percent }
\newlabel{fig:NN_layers}{{15}{25}{Neural networks with different number of layers, but same number of total nodes. Top left has 1 hidden layer, top right has 2. Bottom left has 3, while bottom right has 4. All have a total of 20 nodes.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Test MSE as function of learning rate $\eta $ and regularization parameter $\lambda $. The two blank squares in the top right corner correspond to MSE higher than 1, and thus not shown.\relax }}{26}{figure.caption.17}\protected@file@percent }
\newlabel{fig:NN_lambda}{{16}{26}{Test MSE as function of learning rate $\eta $ and regularization parameter $\lambda $. The two blank squares in the top right corner correspond to MSE higher than 1, and thus not shown.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Wisconsin Breast Cancer Data}{27}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Feed Forward Neural Network}{27}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Test accuracy using sigmoid as the activation function, with 2 hidden layers with 10 nodes each. The y-axis shows to base 10 logarithm of the $\eta $ values. For $\eta =1$, we get overflow, so no value is shown. \relax }}{27}{figure.caption.18}\protected@file@percent }
\newlabel{fig:NN_Cancer_sigmoid}{{17}{27}{Test accuracy using sigmoid as the activation function, with 2 hidden layers with 10 nodes each. The y-axis shows to base 10 logarithm of the $\eta $ values. For $\eta =1$, we get overflow, so no value is shown. \relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Test accuracy using sigmoid as the activation function, with dynamic learning rate. For $\eta =1$, we get overflow, so no value is shown. \relax }}{28}{figure.caption.19}\protected@file@percent }
\newlabel{fig:NN_Cancer_sigmoid_dynam_eta}{{18}{28}{Test accuracy using sigmoid as the activation function, with dynamic learning rate. For $\eta =1$, we get overflow, so no value is shown. \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Test accuracy using RELU (top) and Leaky RELU (bottom) as the activation functions. For $\eta =0.1$, we get overflow, so no value is shown. \relax }}{29}{figure.caption.20}\protected@file@percent }
\newlabel{fig:NN_Cancer_relu_leaky}{{19}{29}{Test accuracy using RELU (top) and Leaky RELU (bottom) as the activation functions. For $\eta =0.1$, we get overflow, so no value is shown. \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Test accuracy using sigmoid as the activation function, with 2 hidden layers with 10 nodes each. For $\eta =1$, we get overflow, so no value is shown. \relax }}{30}{figure.caption.21}\protected@file@percent }
\newlabel{fig:NN_Cancer_eta_lambda}{{20}{30}{Test accuracy using sigmoid as the activation function, with 2 hidden layers with 10 nodes each. For $\eta =1$, we get overflow, so no value is shown. \relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Logistic Regression}{30}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Neural networks with different number of layers. Top left has 1 hidden layer with 20 nodes, top right has 2, each with 20 nodes. Bottom left has 3 hidden layers, each with 20 nodes, while bottom right has 4, each with 20 nodes.\relax }}{31}{figure.caption.22}\protected@file@percent }
\newlabel{fig:NN_class_layers}{{21}{31}{Neural networks with different number of layers. Top left has 1 hidden layer with 20 nodes, top right has 2, each with 20 nodes. Bottom left has 3 hidden layers, each with 20 nodes, while bottom right has 4, each with 20 nodes.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces These two figures illustrates how the accuracy score evolves as a function of epochs ($x$-axis) and learning rate $\eta $ ($y$-axis), using logistic regression. An accuracy score of one corresponds to 100\% correct classifications.\relax }}{32}{figure.caption.23}\protected@file@percent }
\newlabel{fig:logreg_eta_epoch}{{22}{32}{These two figures illustrates how the accuracy score evolves as a function of epochs ($x$-axis) and learning rate $\eta $ ($y$-axis), using logistic regression. An accuracy score of one corresponds to 100\% correct classifications.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Here we have plotted the accuracy for the test data, as a function of both learning rate $\eta $ ($y$-axis) and logarithm of hyper parameter $\lambda $ ($x$-axis), using logistic regression. An accuracy score of one corresponds to 100\% correct classification.\relax }}{33}{figure.caption.24}\protected@file@percent }
\newlabel{fig:logreg_eta_lambda}{{23}{33}{Here we have plotted the accuracy for the test data, as a function of both learning rate $\eta $ ($y$-axis) and logarithm of hyper parameter $\lambda $ ($x$-axis), using logistic regression. An accuracy score of one corresponds to 100\% correct classification.\relax }{figure.caption.24}{}}
\citation{project1}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{34}{section.5}\protected@file@percent }
\citation{project1}
\bibstyle{plain}
\bibdata{refs}
\bibcite{xavier}{{1}{}{{}}{{}}}
\bibcite{He}{{2}{}{{}}{{}}}
\bibcite{2019}{{3}{}{{}}{{}}}
\bibcite{project1}{{4}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{35}{section.6}\protected@file@percent }
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{39}
